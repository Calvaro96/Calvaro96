{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10542659,"datasetId":6523125,"databundleVersionId":10874454}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets wandb torch psutil scikit-learn sacremoses\n\n# Import required modules\nimport wandb\nimport psutil\nimport pandas as pd\nimport torch\nimport sacremoses\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"152d2e01-36d3-42ea-9bc6-79a6fc710d9b","_cell_guid":"d55cf96f-639b-4b90-91f4-712424a9993f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:00:33.856351Z","iopub.execute_input":"2025-01-21T23:00:33.856755Z","iopub.status.idle":"2025-01-21T23:01:03.198730Z","shell.execute_reply.started":"2025-01-21T23:00:33.856715Z","shell.execute_reply":"2025-01-21T23:01:03.198070Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Initialize W&B\nwandb.login(key=\"d81dc4e998e5ec3da77e251749a564584ae99b50\")\nwandb.init(\n    project=\"herbert-hate-detector\",  # Replace with your project name\n    entity=\"m-baloniakk\",            # Replace with your W&B username/entity\n    name=\"fine-tune-ban-pl-2\",       # Optional: Run name\n    config={                         # Hyperparameters for tracking\n        \"learning_rate\": 2e-5,\n        \"epochs\": 3,\n        \"batch_size\": 16,\n        \"model\": \"HerBERT\",\n        \"dataset\": \"BAN_PL_1\"\n    },\n    settings=wandb.Settings(start_method=\"fork\")  # Fixes some runtime issues with multiprocessing\n)\n\n# Load dataset\ndataset_path = \"/kaggle/input/ban-pl-1/BAN-PL.csv\"  # Update this path\ndata = pd.read_csv(dataset_path, encoding=\"utf-8\")\n\n# Preview the dataset\nprint(data.head())","metadata":{"_uuid":"c49e0095-90f4-4a23-bb25-f7adffc34d26","_cell_guid":"359bf6e4-6b6c-458b-bece-7cf95723dc8c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:01:03.199798Z","iopub.execute_input":"2025-01-21T23:01:03.200408Z","iopub.status.idle":"2025-01-21T23:01:15.564179Z","shell.execute_reply.started":"2025-01-21T23:01:03.200385Z","shell.execute_reply":"2025-01-21T23:01:15.563328Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-baloniak\u001b[0m (\u001b[33mm-baloniakk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250121_230109-iuywf6w5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/m-baloniakk/herbert-hate-detector/runs/iuywf6w5' target=\"_blank\">fine-tune-ban-pl-2</a></strong> to <a href='https://wandb.ai/m-baloniakk/herbert-hate-detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/m-baloniakk/herbert-hate-detector' target=\"_blank\">https://wandb.ai/m-baloniakk/herbert-hate-detector</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/m-baloniakk/herbert-hate-detector/runs/iuywf6w5' target=\"_blank\">https://wandb.ai/m-baloniakk/herbert-hate-detector/runs/iuywf6w5</a>"},"metadata":{}},{"name":"stdout","text":"     id                                               Text  Class\n0  2200  \\n\\n\\n            Polska wtedy oficjalnie powi...      0\n1  2201  \\n  Gigantyczna różnica\\n\\n{USERNAME}: biorac ...      0\n2  2202  \\n\\n            {USERNAME}: Moj kumpel budowla...      0\n3  2203         kura, rzodkiewka za 3pln to nie jest tanio      0\n4  2204  {USERNAME}: większość nie idzie w marszu za PO...      0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load HerBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n\n# Tokenize text\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\n# Split dataset into training and validation sets\ntexts = data[\"Text\"].tolist()\nlabels = data[\"Class\"].tolist()\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, labels, test_size=0.2, random_state=42\n)","metadata":{"_uuid":"929c799f-0238-4fe2-a607-79bb8cd8969f","_cell_guid":"2a86bae2-5373-4bcf-bed9-8a536e937e67","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:01:15.566090Z","iopub.execute_input":"2025-01-21T23:01:15.566374Z","iopub.status.idle":"2025-01-21T23:01:16.782215Z","shell.execute_reply.started":"2025-01-21T23:01:15.566352Z","shell.execute_reply":"2025-01-21T23:01:16.781511Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7969df643c3443ebae874a32cf8b33cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55206c8f043b4e93a66f1b7a1e19527a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/907k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5bb3dfa3b5145c4b386304ab8f3d527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/556k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5038a87a216448b88f7df9527a31520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e983aea7868d4211bd7c8a541a9356d7"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Tokenize splits\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n\n# Define PyTorch dataset class\nclass BANPLDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n# Create PyTorch datasets\ntrain_dataset = BANPLDataset(train_encodings, train_labels)\nval_dataset = BANPLDataset(val_encodings, val_labels)","metadata":{"_uuid":"568787be-7e13-47f7-ac9b-718f4202efea","_cell_guid":"30739334-f4d2-4c5b-a0a1-fa28aa55576f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:01:16.783352Z","iopub.execute_input":"2025-01-21T23:01:16.783557Z","iopub.status.idle":"2025-01-21T23:01:20.751362Z","shell.execute_reply.started":"2025-01-21T23:01:16.783541Z","shell.execute_reply":"2025-01-21T23:01:20.750460Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load the pre-trained HerBERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"allegro/herbert-base-cased\", num_labels=2)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",          # output directory\n    evaluation_strategy=\"steps\",     # how often to evaluate\n    logging_dir=\"./logs\",            # directory for storing logs\n    logging_steps=50,               # log every 100 steps\n    save_steps=50,                  # save model every 111 steps\n    per_device_train_batch_size=128,  # batch size per device during training\n    per_device_eval_batch_size=128,   # batch size for evaluation\n    gradient_accumulation_steps=2,\n    fp16=True,\n    num_train_epochs=10,              # number of training epochs\n    weight_decay=0.01,               # strength of weight decay\n    push_to_hub=False,               # don't push to HuggingFace hub\n    load_best_model_at_end=True,     # load the best model when finished\n    report_to=\"wandb\",               # log to W&B\n    remove_unused_columns=False,     # ensure all columns are used\n    metric_for_best_model=\"accuracy\", # metric to track for best model\n    greater_is_better=True,          # whether higher metrics are better\n)","metadata":{"_uuid":"c7da02f3-65a2-444e-9261-70818d76166a","_cell_guid":"c61ed260-3e6d-4495-8700-2051b7cbf2ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:21:03.162037Z","iopub.execute_input":"2025-01-21T23:21:03.162325Z","iopub.status.idle":"2025-01-21T23:21:04.033207Z","shell.execute_reply.started":"2025-01-21T23:21:03.162303Z","shell.execute_reply":"2025-01-21T23:21:04.031977Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Define the metrics function\ndef compute_metrics(p):\n    preds, labels = p\n    preds = preds.argmax(axis=-1)  # Get the predicted class labels (max probability)\n\n    accuracy = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"_uuid":"78344533-805c-4f4b-998c-6bbffed51b5d","_cell_guid":"bf029d99-d532-4a89-a77a-4d3fa647880c","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T23:01:25.421598Z","iopub.execute_input":"2025-01-21T23:01:25.421903Z","iopub.status.idle":"2025-01-21T23:01:25.428277Z","shell.execute_reply.started":"2025-01-21T23:01:25.421863Z","shell.execute_reply":"2025-01-21T23:01:25.427273Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!rm -r /kaggle/working/results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T23:21:14.192406Z","iopub.execute_input":"2025-01-21T23:21:14.192935Z","iopub.status.idle":"2025-01-21T23:21:14.381218Z","shell.execute_reply.started":"2025-01-21T23:21:14.192881Z","shell.execute_reply":"2025-01-21T23:21:14.380129Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                         # your model\n    args=training_args,                  # training arguments\n    train_dataset=train_dataset,         # your training dataset\n    eval_dataset=val_dataset,           # your eval dataset\n    tokenizer=tokenizer,                 # tokenizer\n    compute_metrics=compute_metrics,     # metrics function (e.g., accuracy)\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\nresults = trainer.evaluate()","metadata":{"_uuid":"1528ca11-0218-4120-902f-540fdbe2b9c8","_cell_guid":"510f5b85-d22c-4595-97a5-ab93fc25448d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-21T23:21:21.861309Z","iopub.execute_input":"2025-01-21T23:21:21.861651Z","iopub.status.idle":"2025-01-21T23:58:06.180604Z","shell.execute_reply.started":"2025-01-21T23:21:21.861624Z","shell.execute_reply":"2025-01-21T23:58:06.179990Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"<ipython-input-11-a148b5b84f0e>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [370/370 36:18, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.367400</td>\n      <td>0.226217</td>\n      <td>0.908542</td>\n      <td>0.908537</td>\n      <td>0.908590</td>\n      <td>0.908542</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.184400</td>\n      <td>0.217282</td>\n      <td>0.917500</td>\n      <td>0.917423</td>\n      <td>0.918868</td>\n      <td>0.917500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.125200</td>\n      <td>0.227634</td>\n      <td>0.922500</td>\n      <td>0.922486</td>\n      <td>0.922911</td>\n      <td>0.922500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.077800</td>\n      <td>0.271127</td>\n      <td>0.921875</td>\n      <td>0.921868</td>\n      <td>0.921971</td>\n      <td>0.921875</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.049200</td>\n      <td>0.285220</td>\n      <td>0.925833</td>\n      <td>0.925834</td>\n      <td>0.925839</td>\n      <td>0.925833</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.032100</td>\n      <td>0.326860</td>\n      <td>0.922917</td>\n      <td>0.922912</td>\n      <td>0.922971</td>\n      <td>0.922917</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.022000</td>\n      <td>0.331320</td>\n      <td>0.924583</td>\n      <td>0.924580</td>\n      <td>0.924616</td>\n      <td>0.924583</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n<ipython-input-4-bfd04aae6458>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:18]\n    </div>\n    "},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Finish W&B run\nwandb.finish()","metadata":{"_uuid":"02c8fb36-be28-47e7-ad98-bf3b7a6f5001","_cell_guid":"26f6a6f1-4237-4f6f-9823-0b0f0957a603","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-22T00:13:23.645324Z","iopub.execute_input":"2025-01-22T00:13:23.645697Z","iopub.status.idle":"2025-01-22T00:13:24.855377Z","shell.execute_reply.started":"2025-01-22T00:13:23.645669Z","shell.execute_reply":"2025-01-22T00:13:24.854745Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▁▆▂▅▇▇█▇██</td></tr><tr><td>eval/f1</td><td>▁▆▁▆▂▅▇▇█▇██</td></tr><tr><td>eval/loss</td><td>▂▁▄▅▂▁▂▅▅██▅</td></tr><tr><td>eval/precision</td><td>▁▆▂▆▂▅▇▇█▇██</td></tr><tr><td>eval/recall</td><td>▁▆▁▆▂▅▇▇█▇██</td></tr><tr><td>eval/runtime</td><td>▆▃▁▇▃▇▇▇▇▃█▇</td></tr><tr><td>eval/samples_per_second</td><td>▃▆█▂▆▂▂▂▂▆▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▂▇█▂▇▂▂▂▂▇▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▁▁▂▂▃▃▄▄▅▅▆▆████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▁▁▂▂▃▃▄▄▅▅▆▆████</td></tr><tr><td>train/grad_norm</td><td>█▁█▂▇▄▂▂▁▅▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂█▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.92583</td></tr><tr><td>eval/f1</td><td>0.92583</td></tr><tr><td>eval/loss</td><td>0.28522</td></tr><tr><td>eval/precision</td><td>0.92584</td></tr><tr><td>eval/recall</td><td>0.92583</td></tr><tr><td>eval/runtime</td><td>19.7487</td></tr><tr><td>eval/samples_per_second</td><td>243.054</td></tr><tr><td>eval/steps_per_second</td><td>0.962</td></tr><tr><td>total_flos</td><td>1.230938761396224e+16</td></tr><tr><td>train/epoch</td><td>9.74667</td></tr><tr><td>train/global_step</td><td>370</td></tr><tr><td>train/grad_norm</td><td>234497.45312</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.022</td></tr><tr><td>train_loss</td><td>0.11698</td></tr><tr><td>train_runtime</td><td>2183.5939</td></tr><tr><td>train_samples_per_second</td><td>87.928</td></tr><tr><td>train_steps_per_second</td><td>0.169</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fine-tune-ban-pl-2</strong> at: <a href='https://wandb.ai/m-baloniakk/herbert-hate-detector/runs/iuywf6w5' target=\"_blank\">https://wandb.ai/m-baloniakk/herbert-hate-detector/runs/iuywf6w5</a><br> View project at: <a href='https://wandb.ai/m-baloniakk/herbert-hate-detector' target=\"_blank\">https://wandb.ai/m-baloniakk/herbert-hate-detector</a><br>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250121_230109-iuywf6w5/logs</code>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"!zip -r results.zip results/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T00:19:10.079655Z","iopub.execute_input":"2025-01-22T00:19:10.080002Z","iopub.status.idle":"2025-01-22T00:28:04.417655Z","shell.execute_reply.started":"2025-01-22T00:19:10.079974Z","shell.execute_reply":"2025-01-22T00:28:04.416642Z"}},"outputs":[{"name":"stdout","text":"  adding: results/ (stored 0%)\n  adding: results/checkpoint-350/ (stored 0%)\n  adding: results/checkpoint-350/trainer_state.json (deflated 75%)\n  adding: results/checkpoint-350/model.safetensors (deflated 9%)\n  adding: results/checkpoint-350/scheduler.pt (deflated 55%)\n  adding: results/checkpoint-350/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-350/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-350/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-350/training_args.bin (deflated 51%)\n  adding: results/checkpoint-350/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-350/merges.txt (deflated 60%)\n  adding: results/checkpoint-350/vocab.json (deflated 62%)\n  adding: results/checkpoint-350/config.json (deflated 50%)\n  adding: results/checkpoint-350/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-370/ (stored 0%)\n  adding: results/checkpoint-370/trainer_state.json (deflated 75%)\n  adding: results/checkpoint-370/model.safetensors (deflated 9%)\n  adding: results/checkpoint-370/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-370/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-370/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-370/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-370/training_args.bin (deflated 51%)\n  adding: results/checkpoint-370/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-370/merges.txt (deflated 60%)\n  adding: results/checkpoint-370/vocab.json (deflated 62%)\n  adding: results/checkpoint-370/config.json (deflated 50%)\n  adding: results/checkpoint-370/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-250/ (stored 0%)\n  adding: results/checkpoint-250/trainer_state.json (deflated 73%)\n  adding: results/checkpoint-250/model.safetensors (deflated 9%)\n  adding: results/checkpoint-250/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-250/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-250/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-250/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-250/training_args.bin (deflated 51%)\n  adding: results/checkpoint-250/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-250/merges.txt (deflated 60%)\n  adding: results/checkpoint-250/vocab.json (deflated 62%)\n  adding: results/checkpoint-250/config.json (deflated 50%)\n  adding: results/checkpoint-250/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-50/ (stored 0%)\n  adding: results/checkpoint-50/trainer_state.json (deflated 59%)\n  adding: results/checkpoint-50/model.safetensors (deflated 9%)\n  adding: results/checkpoint-50/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-50/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-50/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-50/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-50/training_args.bin (deflated 51%)\n  adding: results/checkpoint-50/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-50/merges.txt (deflated 60%)\n  adding: results/checkpoint-50/vocab.json (deflated 62%)\n  adding: results/checkpoint-50/config.json (deflated 50%)\n  adding: results/checkpoint-50/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-150/ (stored 0%)\n  adding: results/checkpoint-150/trainer_state.json (deflated 68%)\n  adding: results/checkpoint-150/model.safetensors (deflated 9%)\n  adding: results/checkpoint-150/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-150/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-150/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-150/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-150/training_args.bin (deflated 51%)\n  adding: results/checkpoint-150/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-150/merges.txt (deflated 60%)\n  adding: results/checkpoint-150/vocab.json (deflated 62%)\n  adding: results/checkpoint-150/config.json (deflated 50%)\n  adding: results/checkpoint-150/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-200/ (stored 0%)\n  adding: results/checkpoint-200/trainer_state.json (deflated 71%)\n  adding: results/checkpoint-200/model.safetensors (deflated 9%)\n  adding: results/checkpoint-200/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-200/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-200/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-200/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-200/training_args.bin (deflated 51%)\n  adding: results/checkpoint-200/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-200/merges.txt (deflated 60%)\n  adding: results/checkpoint-200/vocab.json (deflated 62%)\n  adding: results/checkpoint-200/config.json (deflated 50%)\n  adding: results/checkpoint-200/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-100/ (stored 0%)\n  adding: results/checkpoint-100/trainer_state.json (deflated 64%)\n  adding: results/checkpoint-100/model.safetensors (deflated 9%)\n  adding: results/checkpoint-100/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-100/tokenizer.json (deflated 82%)\n  adding: results/checkpoint-100/tokenizer_config.json (deflated 76%)\n  adding: results/checkpoint-100/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-100/training_args.bin (deflated 51%)\n  adding: results/checkpoint-100/optimizer.pt (deflated 17%)\n  adding: results/checkpoint-100/merges.txt (deflated 60%)\n  adding: results/checkpoint-100/vocab.json (deflated 62%)\n  adding: results/checkpoint-100/config.json (deflated 50%)\n  adding: results/checkpoint-100/special_tokens_map.json (deflated 53%)\n  adding: results/checkpoint-300/ (stored 0%)\n  adding: results/checkpoint-300/trainer_state.json (deflated 74%)\n  adding: results/checkpoint-300/model.safetensors\nzip I/O error: No space left on device\nzip error: Output file write failure (write error on zip file)\n","output_type":"stream"}],"execution_count":15}]}